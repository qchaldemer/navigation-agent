{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project, I have trained an agent to navigate (and collect bananas!) in a large, square world.\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of the agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction. Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:\n",
    "\n",
    "0 - move forward.\n",
    "1 - move backward.\n",
    "2 - turn left.\n",
    "3 - turn right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algoritm\n",
    "\n",
    "### Background\n",
    "- The goal of the agent is to maximize the expected value of future. I made the assumption that future rewards are discounted by a factor gamma (see details below)\n",
    "- We consider sequences of actions and observations, st =x1, a1, x2, ..., at−1, xt, and learn game strategies that depend upon these sequences. All sequences are assumed to terminate in a finite number of time-steps, which corresponds to a finite Markov Decision process\n",
    "- We define the optimal action-value function Q∗(s, a) as the maximum expected return achievable by following any strategy, after seeing some sequences and then taking some action a, Q∗(s, a) = maxπ E [Rt|st = s, at = a, π], where π is a policy mapping sequences to actions (or distributions over actions) as described in the Playing Atari with Deep Reinforcement Learning paper from the University of Toronto (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "- The optimal action-value function obeys to the Bellman equation as described in the same paper, which allow to estimate the action-value function, by using the Bellman equation as an iterative update\n",
    "- In this specific case to approximate the action value function, we use a non linear function approximator with a neural network (Q network) for which the hyperparameters are detailed in the section below\n",
    "- I used stochastic gradient descent to train the network which uses the same loss function as (2) in the paper mentioned above.\n",
    "- To avoid strong correlations between consequentive samples, I am using experience replay to save episodes in a replay memory as mentioned in the Human-level control through deep reinforcement learning article (https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "- Similarly, I am using fixed Q targets to avoid updating a guess with a guess and making the algorithm more stable described in the same paper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Details of the implementation\n",
    "- I have used a Deep Q Learning to solve this task with the following hyperparameters:\n",
    "    - eps_start=1.0, eps_end=0.01, eps_decay=0.995\n",
    "- The DQN agent has the following hyperparamters:\n",
    "    - BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "    - BATCH_SIZE = 64         # minibatch size\n",
    "    - GAMMA = 0.99            # discount factor\n",
    "    - TAU = 1e-3              # for soft update of target parameters\n",
    "    - LR = 5e-4               # learning rate \n",
    "    - UPDATE_EVERY = 4        # how often to update the network\n",
    "- The neural network has the following specificities:\n",
    "    - FC NN with 3 layers of 128, 64 and 4 (action size)\n",
    "    - Two first layers are activated with Relu function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "- The agent reached an average of 15.02 over 100 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areas of improvements\n",
    "\n",
    "- To improve the performance of the agents, I could use:\n",
    "    1.  double DQN as described in this paper https://arxiv.org/abs/1509.06461\n",
    "    2. Prioritized experience replay to learn from the most important yet infrequent experiences as described in this research paper (https://arxiv.org/abs/1511.05952)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
